import numpy as np
import pandas as pd
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import LabelEncoder
import tensorflow as tf
from tensorflow.keras import layers

# Load the data
data = pd.read_csv("Anomaly dataset for deep learning.csv")
data.head()

# Load the data
data = pd.read_csv("Anomaly dataset for deep learning.csv")
data.head()

#label encoding 
for column in data.columns:
    if data[column].dtype == np.object:
        encoded = LabelEncoder()
        
        encoded.fit(data[column])
        data[column] = encoded.transform(data[column])
data.head()

### Checking Null values
list1 = []
for i in data.columns:
  null = sum(pd.isnull(data[i]))
  null1 = i+' - '+str(null)
  list1.append(null1)
list1

# Preprocess the data
mean = np.mean(data, axis=0)
std = np.std(data, axis=0)
data = (data - mean) / std
data.head()

# Build the generator model - GAN
generator = tf.keras.Sequential()
generator.add(tf.keras.layers.Dense(64, input_dim=100, activation="relu"))
generator.add(tf.keras.layers.Dense(128, activation="relu"))
generator.add(tf.keras.layers.Dense(data.shape[1], activation="sigmoid"))

# Build the discriminator model
discriminator = tf.keras.Sequential()
discriminator.add(tf.keras.layers.Dense(128, input_dim=data.shape[1], activation="relu"))
discriminator.add(tf.keras.layers.Dense(64, activation="relu"))
discriminator.add(tf.keras.layers.Dense(1, activation="sigmoid"))

# Compile the discriminator
discriminator.compile(optimizer="adam", loss="binary_crossentropy")
# Freeze the discriminator weights
discriminator.trainable = False

# Build the combined model
inputs = tf.keras.layers.Input(shape=(100,))
generated_data = generator(inputs)
validity = discriminator(generated_data)
gan = tf.keras.models.Model(inputs, validity)
gan.compile(optimizer="adam", loss="binary_crossentropy")

# Train the GAN
for epoch in range(100):
    # Generate synthetic data
    noise = np.random.normal(0, 1, (data.shape[0], 100))
    synthetic_data = generator.predict(noise)
    # Train the discriminator on real data and synthetic data
    real_labels = np.ones((data.shape[0], 1))
    synthetic_labels = np.zeros((data.shape[0], 1))
    d_loss_real = discriminator.train_on_batch(data, real_labels)
    d_loss_synthetic = discriminator.train_on_batch(synthetic_data, synthetic_labels)
    # Train the generator
    g_loss = gan.train_on_batch(noise, real_labels)
    # Print the loss values for each epoch
    print("Epoch: %d, D Loss (real): %.4f, D Loss (synthetic): %.4f, G Loss: %.4f" % (epoch + 1, d_loss_real, d_loss_synthetic, g_loss))

# Evaluate the synthetic data using the discriminator
scores = discriminator.predict(data)
# Threshold the scores to identify anomalies
threshold = np.mean(scores) + 3 * np.std(scores)
anomalies = np.where(reconstruction_error > threshold)

anomaly_data = data.iloc[[i for i in anomalies[0]]]
anomaly_data
